{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a62d887-01aa-4980-9220-3bf28c077504",
   "metadata": {},
   "source": [
    "### Домашнее задание Transformers Training (50 баллов)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f31d7452-febe-4d25-a9c1-005fcc26b35b",
   "metadata": {},
   "source": [
    "В этом домашнем задании требуется обучить несколько Transformer-based моделей в задаче машинного перевода. Для обучения можно воспользоваться текущим проектом, так и реализовать свой пайплайн обучения. Если будете использовать проект, теги **TODO** проекта отмечают, какие компоненты надо реализовать.\n",
    "В ноутбуке нужно только отобразить результаты обучения и выводы. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Ваш код обучения нужно выложить на ваш github, в строке ниже дать ссылку на него. В первую очередь будут оцениваться результаты в ноутбуке, код нужен для проверки адекватности результатов. \n",
    "\n",
    "Обучать модели до конца не нужно, только для демонстрации, что модель обучается и рабочая - снижение val_loss, рост bleu_score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a691e7b9-6538-42b1-96b8-838f2efab4af",
   "metadata": {},
   "source": [
    "#### Сcылка на ваш github с проектом(вставить свой) - https://github.com/easykvasha/pytorch-machine-translation\n",
    "\n",
    "Ноутбук с результатами выкладывать на ваш **google диск** курса. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbca1e62-b210-4426-9854-55b652417a4b",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "`\n",
    "wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip\n",
    "`\n",
    "\n",
    "Модели нужно обучить на задаче перевода с английского на русский. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710e84de-f7a4-46da-a611-74c90a67e24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krilo\\anaconda3\\envs\\DiffuSeq_env\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:162: UserWarning: pylab import has clobbered these variables: ['plt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc772a6e-7d1a-4d8d-8024-0454a948835b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Обучение Seq2seq Transformer модель(25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Transformer. В качестве блока трансформера можно использовать https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html. В качестве токенизатора воспользуйтесь HuggingFace токенизатор для source/target языков - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. \n",
    "\n",
    "Не забудьте остальные элементы модели:\n",
    "* Мы можем использовать 1 трансформер как энкодер - декодером будет выступать линейный слой. \n",
    "* Обучите свой BPE токенизатор - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "* Матрицу эмбеддингов токенов\n",
    "* Матрицу позицонных эмбеддингов\n",
    "* Линейный слой проекции в target словарь\n",
    "* Функцию маскирования будущих состояний attention, так как модель авто-регрессионна\n",
    "* Learning rate schedualer\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, размерность скрытого слоя, количетсво слоев\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1fdb6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH_REGEX = re.compile(r\"[+-]?\\b(\\d+([.]\\d*)?([eE][+-]?\\d+)?|[.]\\d+([eE][+-]?\\d+)?)\\b\")\n",
    "\n",
    "def extract_values(string):\n",
    "    result = dict()\n",
    "    params = ['train_loss', 'val_loss', 'bleu_score']\n",
    "    if \"train_loss\" in string:\n",
    "        found_vals = [re_match[0] for re_match in re.findall(MATCH_REGEX, string)]\n",
    "    \n",
    "        if len(params) == len(found_vals):\n",
    "            for name, val in zip(params, found_vals):\n",
    "                result[name] = float(val)\n",
    "    return result\n",
    "\n",
    "def plot_results(train_loss_list, val_loss_list, val_bleu_list, run_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    \n",
    "    ax1.plot(range(len(train_loss_list)), train_loss_list, label='train loss')\n",
    "    ax1.plot(range(len(val_loss_list)), val_loss_list, label='val loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(range(len(val_bleu_list)), val_bleu_list, label='val bleu')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('BLEU')\n",
    "    ax2.legend()\n",
    "    \n",
    "    fig.suptitle(run_name, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d45f6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = './training_logs_seq2seq'\n",
    "progress_file_path = os.path.join(DATAPATH, \"progress_log.txt\")\n",
    "file_content = open(progress_file_path, 'r').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "59b26f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80, 80)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list, val_loss_list, val_bleu_list = [], [], []\n",
    "for line in file_content:\n",
    "    d = extract_values(line)\n",
    "    if len(d) > 0:\n",
    "        train_loss_list.append(d['train_loss'])\n",
    "        val_loss_list.append(d['val_loss'])\n",
    "        val_bleu_list.append(d['bleu_score'])\n",
    "    \n",
    "        \n",
    "len(train_loss_list), len(val_loss_list), len(val_bleu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bf2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(train_loss_list, val_loss_list, val_bleu_list, run_name='Seq2Seq')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "245297ae-d62b-4227-9312-ccff5a1c13c9",
   "metadata": {},
   "source": [
    "### Fine-tune pretrained T5 (25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Pretrained T5. Воспользуйтесь https://huggingface.co/docs/transformers/model_doc/t5 предобученной моделью. В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Не забудьте важные аспекты обучения модели:\n",
    "* Взять готовый t5 токенизатор\n",
    "* Resize matrix embedding - скорей всего ваша матрица эмбеддингов не будет включать эмбеддинги из вашего сета. Пример обновления матрицы эмбеддингов тут тут https://github.com/runnerup96/Transformers-Tuning/blob/main/t5_encoder_decoder.py\n",
    "* Learning rate schedualer/Adafactor with constant learning rate\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, pretrained model name\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ee3b967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH_REGEX = re.compile(r\"[+-]?\\b(\\d+([.]\\d*)?([eE][+-]?\\d+)?|[.]\\d+([eE][+-]?\\d+)?)\\b\")\n",
    "\n",
    "def extract_values(string):\n",
    "    result = dict()\n",
    "    params = ['train_loss', 'val_loss', 'bleu_score']\n",
    "    if \"train_loss\" in string:\n",
    "        found_vals = [re_match[0] for re_match in re.findall(MATCH_REGEX, string)]\n",
    "    \n",
    "        if len(params) == len(found_vals):\n",
    "            for name, val in zip(params, found_vals):\n",
    "                result[name] = float(val)\n",
    "    return result\n",
    "\n",
    "def plot_results(train_loss_list, val_loss_list, val_bleu_list, run_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    \n",
    "    ax1.plot(range(len(train_loss_list)), train_loss_list, label='train loss')\n",
    "    ax1.plot(range(len(val_loss_list)), val_loss_list, label='val loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(range(len(val_bleu_list)), val_bleu_list, label='val bleu')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('BLEU')\n",
    "    ax2.legend()\n",
    "    \n",
    "    fig.suptitle(run_name, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1d896a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = './training_logs_seq2seq_T5'\n",
    "progress_file_path = os.path.join(DATAPATH, \"progress_log.txt\")\n",
    "file_content = open(progress_file_path, 'r').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bbe480c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 158, 158)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list, val_loss_list, val_bleu_list = [], [], []\n",
    "for line in file_content:\n",
    "    d = extract_values(line)\n",
    "    if len(d) > 0:\n",
    "        train_loss_list.append(d['train_loss'])\n",
    "        val_loss_list.append(d['val_loss'])\n",
    "        val_bleu_list.append(d['bleu_score'])\n",
    "    \n",
    "        \n",
    "len(train_loss_list), len(val_loss_list), len(val_bleu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737686f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(train_loss_list, val_loss_list, val_bleu_list, run_name='Seq2Seq_T5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
